<p align="center">
  <strong>Plataforma de GeraÃ§Ã£o de Dados SintÃ©ticos para MÃºltiplos DomÃ­nios</strong>
</p>

<p align="center">
  Desenvolvido por <a href="https://github.com/intelium-labs">Intelium Labs</a>
</p>

---

## Sobre o Projeto

O **Data-Gen** Ã© uma plataforma extensÃ­vel de geraÃ§Ã£o de dados sintÃ©ticos projetada para simular cenÃ¡rios realistas de diversos domÃ­nios de negÃ³cio. A plataforma gera dados com integridade referencial, distribuiÃ§Ãµes estatÃ­sticas realistas e formatos brasileiros.

### DomÃ­nios Suportados

| DomÃ­nio | Status | DescriÃ§Ã£o |
|---------|--------|-----------|
| **ServiÃ§os Financeiros** | âœ… DisponÃ­vel | Banco completo: clientes, contas, transaÃ§Ãµes, cartÃµes, emprÃ©stimos, B3 |
| **Varejo** | ğŸ”œ Planejado | Lojas, produtos, vendas, estoque, promoÃ§Ãµes |
| **E-commerce** | ğŸ”œ Planejado | Pedidos, carrinho, pagamentos, entregas, avaliaÃ§Ãµes |
| **TelecomunicaÃ§Ãµes** | ğŸ”œ Planejado | Planos, consumo, faturas, atendimento |
| **SaÃºde** | ğŸ”œ Planejado | Pacientes, consultas, exames, prontuÃ¡rios |
| **LogÃ­stica** | ğŸ”œ Planejado | Entregas, rotas, rastreamento, frotas |

### Casos de Uso

- **Demos e POCs** - Dados realistas para apresentaÃ§Ãµes e provas de conceito
- **Testes de integraÃ§Ã£o** - Dados consistentes e reproduzÃ­veis para pipelines
- **Desenvolvimento** - Ambiente local sem dependÃªncia de dados de produÃ§Ã£o
- **Treinamento de modelos** - Datasets balanceados para ML/AI
- **Benchmarking** - Testes de performance com volumes controlados
- **Data Lakes** - PopulaÃ§Ã£o de ambientes de dados para anÃ¡lise

---

## Arquitetura de Sinks

O Data-Gen suporta mÃºltiplos destinos de dados (sinks) para diferentes casos de uso:

### DisponÃ­veis

| Sink | Tipo | Caso de Uso |
|------|------|-------------|
| **PostgreSQL** | OLTP | Dados mestres, tabelas relacionais |
| **Kafka** | Streaming | Eventos em tempo real, CDC |
| **JSON Files** | Arquivo | ExportaÃ§Ã£o, backups, testes locais |
| **Console** | Debug | Desenvolvimento e debugging |

### Planejados

| Sink | Tipo | Caso de Uso | Prioridade |
|------|------|-------------|------------|
| **MongoDB** | NoSQL | Documentos, dados semi-estruturados | Alta |
| **Parquet/CSV** | Arquivo | Data Lake, anÃ¡lise batch | Alta |
| **S3/GCS/ADLS** | Cloud Storage | Data Lake na nuvem | Alta |
| **Apache Iceberg** | Table Format | Data Lakehouse | MÃ©dia |
| **Delta Lake** | Table Format | Databricks, Spark | MÃ©dia |
| **BigQuery** | Data Warehouse | Analytics GCP | MÃ©dia |
| **Snowflake** | Data Warehouse | Analytics multi-cloud | MÃ©dia |
| **Elasticsearch** | Search | Busca e observabilidade | Baixa |
| **Redis** | Cache | Cache de sessÃ£o, real-time | Baixa |

---

## Funcionalidades

- **Modelos de DomÃ­nio**: Entidades com relacionamentos e validaÃ§Ãµes
- **Dados Realistas**: Formatos brasileiros (CPF, CNPJ, Pix, CEP), distribuiÃ§Ãµes estatÃ­sticas
- **MÃºltiplos Destinos**: Streaming (Kafka), OLTP (PostgreSQL), arquivos, cloud
- **CenÃ¡rios PrÃ©-construÃ­dos**: Fraud detection, loan portfolio, customer 360
- **Integridade Referencial**: ValidaÃ§Ã£o de FKs entre todas as entidades
- **Reprodutibilidade**: Seeds para geraÃ§Ã£o determinÃ­stica
- **ExtensÃ­vel**: FÃ¡cil adiÃ§Ã£o de novos domÃ­nios, geradores e sinks

## DocumentaÃ§Ã£o

| Documento | DescriÃ§Ã£o |
|-----------|-----------|
| [CatÃ¡logo de Dados](docs/data-catalog.md) | ReferÃªncia completa de todos os datasets, campos e lÃ³gica de geraÃ§Ã£o |
| [IngestÃ£o de Dados](docs/data-ingestion.md) | Como carregar dados no PostgreSQL e Kafka (CLI, performance, scripts) |
| [Infraestrutura Docker](docs/docker.md) | Setup do Confluent Platform, Control Center e PostgreSQL |
| [Setup no Windows](docs/windows-setup.md) | Guia para rodar no Windows com Docker Desktop e WSL2 |
| [Roadmap v2](docs/roadmap-v2.md) | Poison pills, padrÃµes realistas, novos produtos e interface TUI |

---

## Quick Start

### InstalaÃ§Ã£o

```bash
# Instalar o pacote
pip install -e .

# Com dependÃªncias de desenvolvimento
pip install -e ".[dev]"
```

### Uso BÃ¡sico

```python
from data_gen.generators.financial import CustomerGenerator, AccountGenerator
from data_gen.store.financial import FinancialDataStore

# Criar store e generators
store = FinancialDataStore()
customer_gen = CustomerGenerator(seed=42)
account_gen = AccountGenerator(seed=42)

# Gerar customers (generate_batch retorna um iterator)
for customer in customer_gen.generate_batch(100):
    store.add_customer(customer)

# Gerar accounts para cada customer
for customer in store.customers.values():
    for account in account_gen.generate_for_customer(
        customer.customer_id,
        customer.created_at,
        customer.monthly_income,
    ):
        store.add_account(account)

print(f"Customers: {len(store.customers)}")
print(f"Accounts: {len(store.accounts)}")
```

### Carregar Dados via CLI

```bash
# Iniciar infraestrutura
docker compose -f docker/docker-compose.yml up -d

# Carregar 500 clientes no PostgreSQL e Kafka
python scripts/load_data.py --customers 500 --create-topics --truncate

# Carregar 10K clientes (modo rÃ¡pido auto-habilitado: COPY + BULK + streaming)
python scripts/load_data.py --customers 10000 --create-topics --truncate

# Streaming em tempo real: 1 hora a 1000 eventos/seg
python scripts/load_data.py --stream --duration 3600 --create-topics --kafka-cluster oss

# Carga paralela para 100K+ clientes
python scripts/load_data_parallel.py --customers 100000 --create-topics --truncate
```

---

## CenÃ¡rios DisponÃ­veis

### Fraud Detection

Gera transaÃ§Ãµes com padrÃµes de fraude para treinar modelos de detecÃ§Ã£o.

```python
from data_gen.scenarios.financial import FraudDetectionScenario
from data_gen.sinks import KafkaSink

scenario = FraudDetectionScenario(
    num_customers=10_000,
    fraud_rate=0.05,
    seed=42,
)
scenario.generate()
scenario.export([KafkaSink("localhost:9092")])
```

### Loan Portfolio

Simula portfÃ³lio de emprÃ©stimos com comportamento de pagamento realista.

```python
from data_gen.scenarios.financial import LoanPortfolioScenario

scenario = LoanPortfolioScenario(
    num_customers=10_000,
    loan_penetration=0.30,
    default_rate=0.05,
    seed=42,
)
scenario.generate()
```

---

## Estrutura do Projeto

```
data-gen/
â”œâ”€â”€ data_gen/
â”‚   â”œâ”€â”€ models/              # Modelos de domÃ­nio (dataclasses)
â”‚   â”‚   â”œâ”€â”€ base.py          # Tipos compartilhados (Address, Event)
â”‚   â”‚   â””â”€â”€ financial/       # DomÃ­nio financeiro (10 entidades)
â”‚   â”œâ”€â”€ generators/          # Geradores de dados
â”‚   â”‚   â””â”€â”€ financial/       # Geradores financeiros
â”‚   â”œâ”€â”€ store/               # Data store in-memory com validaÃ§Ã£o de FK
â”‚   â”œâ”€â”€ sinks/               # Destinos de saÃ­da
â”‚   â”‚   â”œâ”€â”€ kafka.py         # Apache Kafka + Avro + Schema Registry
â”‚   â”‚   â”œâ”€â”€ postgres.py      # PostgreSQL (COPY + parallel loading)
â”‚   â”‚   â”œâ”€â”€ json_file.py     # Arquivos JSON
â”‚   â”‚   â”œâ”€â”€ console.py       # Console (debug)
â”‚   â”‚   â””â”€â”€ serialization.py # SerializaÃ§Ã£o compartilhada
â”‚   â””â”€â”€ scenarios/           # CenÃ¡rios prÃ©-construÃ­dos (fraud, loan, 360)
â”œâ”€â”€ docs/                    # DocumentaÃ§Ã£o em pt-BR
â”œâ”€â”€ docker/                  # Docker Compose (Confluent Platform + PostgreSQL)
â”‚   â””â”€â”€ config/              # Configs do Control Center (Prometheus, alertas)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_data.py         # Carga padrÃ£o (PostgreSQL + Kafka)
â”‚   â”œâ”€â”€ load_data_parallel.py # Carga paralela (multiprocessing)
â”‚   â””â”€â”€ benchmark.py         # Benchmark de performance
â””â”€â”€ tests/                   # Testes unitÃ¡rios (390 tests, 99% coverage)
```

---

## Desenvolvimento

```bash
# Instalar dependÃªncias de dev
pip install -e ".[dev]"

# Rodar testes
pytest

# Formatar cÃ³digo
black data_gen tests
ruff check data_gen tests

# Type check
mypy data_gen
```

## Docker (Confluent Platform + PostgreSQL)

```bash
# Subir toda a stack (Broker, Schema Registry, Connect, ksqlDB, Control Center, PostgreSQL)
docker compose -f docker/docker-compose.yml up -d

# Verificar status
docker compose -f docker/docker-compose.yml ps

# Acessar Control Center Next-Gen
open http://localhost:9021

# Parar e remover volumes (reset total)
docker compose -f docker/docker-compose.yml down -v
```

Veja [Infraestrutura Docker](docs/docker.md) para detalhes completos.

---

## Roadmap

### DomÃ­nio: ServiÃ§os Financeiros âœ…

Status: **DisponÃ­vel**

- [x] Clientes (Customer) com CPF, renda, score de crÃ©dito
- [x] Contas bancÃ¡rias (Account) - corrente, poupanÃ§a, investimentos
- [x] TransaÃ§Ãµes (Transaction) - Pix, TED, depÃ³sitos, saques, boletos
- [x] CartÃµes de crÃ©dito (CreditCard) - Visa, Mastercard, Elo
- [x] Compras no cartÃ£o (CardTransaction) com MCC e parcelas
- [x] EmprÃ©stimos (Loan) - pessoal, imobiliÃ¡rio, veicular
- [x] Parcelas (Installment) com SAC e PRICE
- [x] ImÃ³veis (Property) para financiamento habitacional
- [x] AÃ§Ãµes B3 (Stock) e operaÃ§Ãµes (Trade)
- [x] Sink PostgreSQL
- [x] Sink Kafka com Avro e Schema Registry
- [x] CenÃ¡rio: Fraud Detection
- [x] CenÃ¡rio: Loan Portfolio

### DomÃ­nio: Varejo ğŸ”œ

Status: **Planejado**

- [ ] Lojas (Store) - fÃ­sicas e franquias
- [ ] Produtos (Product) - SKU, categoria, preÃ§o
- [ ] Estoque (Inventory) - nÃ­veis, reposiÃ§Ã£o
- [ ] Vendas (Sale) - PDV, pagamentos
- [ ] Clientes (RetailCustomer) - programa fidelidade
- [ ] PromoÃ§Ãµes (Promotion) - descontos, cupons
- [ ] Fornecedores (Supplier)
- [ ] CenÃ¡rio: AnÃ¡lise de vendas
- [ ] CenÃ¡rio: GestÃ£o de estoque
- [ ] CenÃ¡rio: SegmentaÃ§Ã£o de clientes

### DomÃ­nio: E-commerce ğŸ”œ

Status: **Planejado**

- [ ] UsuÃ¡rios (User) - cadastro, autenticaÃ§Ã£o
- [ ] CatÃ¡logo (Catalog) - produtos, categorias, variaÃ§Ãµes
- [ ] Carrinho (Cart) - itens, abandono
- [ ] Pedidos (Order) - checkout, status
- [ ] Pagamentos (Payment) - cartÃ£o, Pix, boleto
- [ ] Entregas (Shipment) - rastreamento, transportadoras
- [ ] AvaliaÃ§Ãµes (Review) - produtos, vendedores
- [ ] CenÃ¡rio: Funil de conversÃ£o
- [ ] CenÃ¡rio: RecomendaÃ§Ã£o de produtos
- [ ] CenÃ¡rio: PrevisÃ£o de demanda

### Novos Sinks ğŸ”œ

- [ ] **MongoDB** - Documentos e dados semi-estruturados
- [ ] **Parquet/CSV** - ExportaÃ§Ã£o para Data Lake
- [ ] **S3/GCS/ADLS** - Cloud object storage
- [ ] **Apache Iceberg** - Table format para Data Lakehouse
- [ ] **Delta Lake** - Databricks e Spark
- [ ] **BigQuery** - Data Warehouse GCP
- [ ] **Snowflake** - Data Warehouse multi-cloud

### Melhorias Gerais ğŸ”œ

- [ ] **CDC com Debezium** - Captura de mudanÃ§as do PostgreSQL para Kafka
- [ ] **Gerador de PIX em tempo real** - Stream contÃ­nuo de transaÃ§Ãµes
- [ ] **API REST** - Endpoint para geraÃ§Ã£o de dados sob demanda
- [ ] **GeraÃ§Ã£o DistribuÃ­da** - Suporte a Spark para grandes volumes
- [ ] **Dashboard de MÃ©tricas** - VisualizaÃ§Ã£o dos dados gerados
- [ ] **IntegraÃ§Ã£o com Great Expectations** - ValidaÃ§Ã£o de qualidade
- [ ] **IntegraÃ§Ã£o com dbt** - TransformaÃ§Ãµes e lineage

### DomÃ­nios Futuros ğŸ”®

- [ ] **TelecomunicaÃ§Ãµes** - Planos, consumo, faturas
- [ ] **SaÃºde** - Pacientes, consultas, exames
- [ ] **LogÃ­stica** - Entregas, rotas, frotas
- [ ] **Seguros** - ApÃ³lices, sinistros, anÃ¡lise de risco
- [ ] **EducaÃ§Ã£o** - Alunos, cursos, avaliaÃ§Ãµes

---

## Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, leia o guia de contribuiÃ§Ã£o antes de submeter PRs.

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

---

## LicenÃ§a

MIT

---

<p align="center">
  <sub>Feito com â¤ï¸ por <a href="https://github.com/intelium-labs">Intelium Labs</a></sub>
</p>
