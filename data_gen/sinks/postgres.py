"""PostgreSQL sink for exporting data to database."""

import logging
from dataclasses import fields, is_dataclass
from datetime import date, datetime
from decimal import Decimal
from enum import Enum
from typing import Any, Iterator

logger = logging.getLogger(__name__)


class PostgresSink:
    """Output data to PostgreSQL database."""

    # Entity table mapping and insert order (respects FK constraints)
    ENTITY_ORDER = [
        "customers",
        "properties",
        "stocks",
        "accounts",
        "credit_cards",
        "loans",
    ]

    # Note: incremental_id is auto-generated by PostgreSQL SERIAL, not included in INSERT
    TABLE_COLUMNS = {
        "customers": [
            "customer_id",
            "cpf",
            "name",
            "email",
            "phone",
            "street",
            "number",
            "complement",
            "neighborhood",
            "city",
            "state",
            "postal_code",
            "country",
            "monthly_income",
            "employment_status",
            "credit_score",
            "created_at",
            "updated_at",
        ],
        "accounts": [
            "account_id",
            "customer_id",
            "account_type",
            "bank_code",
            "branch",
            "account_number",
            "balance",
            "status",
            "created_at",
            "updated_at",
        ],
        "transactions": [
            "transaction_id",
            "account_id",
            "transaction_type",
            "amount",
            "direction",
            "counterparty_key",
            "counterparty_name",
            "description",
            "timestamp",
            "status",
            "pix_e2e_id",
            "pix_key_type",
            "created_at",
            "updated_at",
        ],
        "credit_cards": [
            "card_id",
            "customer_id",
            "card_number_masked",
            "brand",
            "credit_limit",
            "available_limit",
            "due_day",
            "status",
            "created_at",
            "updated_at",
        ],
        "card_transactions": [
            "transaction_id",
            "card_id",
            "merchant_name",
            "merchant_category",
            "mcc_code",
            "amount",
            "installments",
            "timestamp",
            "status",
            "location_city",
            "location_country",
            "created_at",
            "updated_at",
        ],
        "loans": [
            "loan_id",
            "customer_id",
            "loan_type",
            "principal",
            "interest_rate",
            "term_months",
            "amortization_system",
            "status",
            "disbursement_date",
            "property_id",
            "created_at",
            "updated_at",
        ],
        "installments": [
            "installment_id",
            "loan_id",
            "installment_number",
            "due_date",
            "principal_amount",
            "interest_amount",
            "total_amount",
            "paid_date",
            "paid_amount",
            "status",
            "created_at",
            "updated_at",
        ],
        "properties": [
            "property_id",
            "property_type",
            "street",
            "number",
            "complement",
            "neighborhood",
            "city",
            "state",
            "postal_code",
            "country",
            "appraised_value",
            "area_sqm",
            "registration_number",
            "created_at",
            "updated_at",
        ],
        "stocks": [
            "stock_id",
            "ticker",
            "company_name",
            "sector",
            "segment",
            "current_price",
            "currency",
            "isin",
            "lot_size",
            "created_at",
            "updated_at",
        ],
    }

    def __init__(self, connection_string: str) -> None:
        """Initialize PostgreSQL sink.

        Parameters
        ----------
        connection_string : str
            PostgreSQL connection string (e.g., "postgresql://user:pass@localhost/db").
        """
        try:
            import psycopg

            self.conn = psycopg.connect(connection_string)
            self._psycopg = psycopg
        except ImportError:
            raise ImportError("psycopg is required for PostgresSink. Install with: pip install 'psycopg[binary]'")

        self._counts: dict[str, int] = {}

    def write_batch(
        self, entity_type: str, records: list[Any], use_copy: bool = False
    ) -> None:
        """Write a batch of records to PostgreSQL.

        Parameters
        ----------
        entity_type : str
            Table/entity name.
        records : list[Any]
            Records to insert.
        use_copy : bool
            If True, use COPY protocol for faster bulk loading (default: False).
        """
        if not records:
            return

        if entity_type not in self.TABLE_COLUMNS:
            logger.warning("Unknown entity type: %s", entity_type)
            return

        columns = self.TABLE_COLUMNS[entity_type]

        if use_copy:
            # Stream rows directly to COPY â€” no intermediate list
            self._write_copy_stream(entity_type, columns, records)
        else:
            rows = [self._extract_row(r, columns, entity_type) for r in records]
            self._write_executemany(entity_type, columns, rows)

        self._counts[entity_type] = self._counts.get(entity_type, 0) + len(records)
        logger.info("Inserted %d records into %s", len(records), entity_type)

    def _write_executemany(
        self, table_name: str, columns: list[str], rows: list[tuple]
    ) -> None:
        """Insert rows using executemany (standard method)."""
        sql = self._psycopg.sql.SQL("INSERT INTO {} ({}) VALUES ({})").format(
            self._psycopg.sql.Identifier(table_name),
            self._psycopg.sql.SQL(", ").join(
                self._psycopg.sql.Identifier(col) for col in columns
            ),
            self._psycopg.sql.SQL(", ").join(
                self._psycopg.sql.Placeholder() for _ in columns
            ),
        )
        with self.conn.cursor() as cur:
            cur.executemany(sql, rows)
        self.conn.commit()

    def _write_copy(
        self, table_name: str, columns: list[str], rows: list[tuple]
    ) -> None:
        """Insert pre-extracted rows using COPY protocol."""
        col_ids = self._psycopg.sql.SQL(", ").join(
            self._psycopg.sql.Identifier(col) for col in columns
        )
        copy_sql = self._psycopg.sql.SQL(
            "COPY {} ({}) FROM STDIN"
        ).format(
            self._psycopg.sql.Identifier(table_name),
            col_ids,
        )

        with self.conn.cursor() as cur:
            with cur.copy(copy_sql) as copy:
                for row in rows:
                    copy.write_row(row)
        self.conn.commit()

    def _write_copy_stream(
        self, table_name: str, columns: list[str], records: list[Any]
    ) -> None:
        """Stream records directly to COPY without intermediate list."""
        col_ids = self._psycopg.sql.SQL(", ").join(
            self._psycopg.sql.Identifier(col) for col in columns
        )
        copy_sql = self._psycopg.sql.SQL(
            "COPY {} ({}) FROM STDIN"
        ).format(
            self._psycopg.sql.Identifier(table_name),
            col_ids,
        )

        with self.conn.cursor() as cur:
            with cur.copy(copy_sql) as copy:
                for record in records:
                    copy.write_row(self._extract_row(record, columns, table_name))
        self.conn.commit()

    def write_stream(
        self,
        topic: str,
        generator: Iterator[Any],
        rate_per_second: float,
        duration_seconds: float,
    ) -> None:
        """Stream records to PostgreSQL (batch inserts)."""
        import time

        # Infer entity type from topic
        # e.g., "dev.financial.transactions.created.v1" -> "transactions"
        parts = topic.split(".")
        entity_type = parts[2] if len(parts) > 2 else topic

        if entity_type not in self.TABLE_COLUMNS:
            logger.warning("Cannot infer table from topic: %s", topic)
            return

        start_time = time.time()
        batch: list[Any] = []
        batch_size = 1000

        for record in generator:
            if time.time() - start_time >= duration_seconds:
                break

            batch.append(record)

            if len(batch) >= batch_size:
                self.write_batch(entity_type, batch)
                batch = []

            # Simple rate limiting
            if rate_per_second > 0:
                time.sleep(1.0 / rate_per_second)

        # Write remaining records
        if batch:
            self.write_batch(entity_type, batch)

    def close(self) -> None:
        """Close database connection."""
        self.conn.close()
        logger.info("PostgreSQL sink closed")
        for entity_type, count in self._counts.items():
            logger.info("  %s: %d records", entity_type, count)

    def truncate_tables(self) -> None:
        """Truncate all tables (respecting FK order - reverse of insert order)."""
        # Order matters: truncate tables with FKs first (reverse order)
        truncate_order = [
            "installments",
            "card_transactions",
            "transactions",
            "loans",
            "credit_cards",
            "accounts",
            "stocks",
            "properties",
            "customers",
        ]
        with self.conn.cursor() as cur:
            for table in truncate_order:
                cur.execute(
                    self._psycopg.sql.SQL("TRUNCATE TABLE {} CASCADE").format(
                        self._psycopg.sql.Identifier(table)
                    )
                )
        self.conn.commit()
        logger.info("All tables truncated")

    def create_tables(self) -> None:
        """Create database tables (for development/testing)."""
        ddl = """
        CREATE TABLE IF NOT EXISTS customers (
            incremental_id SERIAL,
            customer_id VARCHAR(36) PRIMARY KEY,
            cpf VARCHAR(14) NOT NULL,
            name VARCHAR(255) NOT NULL,
            email VARCHAR(255),
            phone VARCHAR(20),
            street VARCHAR(255),
            number VARCHAR(20),
            complement VARCHAR(100),
            neighborhood VARCHAR(100),
            city VARCHAR(100),
            state VARCHAR(50),
            postal_code VARCHAR(20),
            country VARCHAR(2) DEFAULT 'BR',
            monthly_income DECIMAL(15,2),
            employment_status VARCHAR(20),
            credit_score INTEGER,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS properties (
            incremental_id SERIAL,
            property_id VARCHAR(36) PRIMARY KEY,
            property_type VARCHAR(20),
            street VARCHAR(255),
            number VARCHAR(20),
            complement VARCHAR(100),
            neighborhood VARCHAR(100),
            city VARCHAR(100),
            state VARCHAR(50),
            postal_code VARCHAR(20),
            country VARCHAR(2) DEFAULT 'BR',
            appraised_value DECIMAL(15,2),
            area_sqm DECIMAL(10,2),
            registration_number VARCHAR(50),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS stocks (
            incremental_id SERIAL,
            stock_id VARCHAR(36) PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            company_name VARCHAR(255) NOT NULL,
            sector VARCHAR(50),
            segment VARCHAR(50),
            current_price DECIMAL(15,2),
            currency VARCHAR(3) DEFAULT 'BRL',
            isin VARCHAR(20),
            lot_size INTEGER DEFAULT 100,
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS accounts (
            incremental_id SERIAL,
            account_id VARCHAR(36) PRIMARY KEY,
            customer_id VARCHAR(36) REFERENCES customers(customer_id),
            account_type VARCHAR(20),
            bank_code VARCHAR(10),
            branch VARCHAR(10),
            account_number VARCHAR(20),
            balance DECIMAL(15,2),
            status VARCHAR(20),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS credit_cards (
            incremental_id SERIAL,
            card_id VARCHAR(36) PRIMARY KEY,
            customer_id VARCHAR(36) REFERENCES customers(customer_id),
            card_number_masked VARCHAR(20),
            brand VARCHAR(20),
            credit_limit DECIMAL(15,2),
            available_limit DECIMAL(15,2),
            due_day INTEGER,
            status VARCHAR(20),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS loans (
            incremental_id SERIAL,
            loan_id VARCHAR(36) PRIMARY KEY,
            customer_id VARCHAR(36) REFERENCES customers(customer_id),
            loan_type VARCHAR(20),
            principal DECIMAL(15,2),
            interest_rate DECIMAL(10,6),
            term_months INTEGER,
            amortization_system VARCHAR(10),
            status VARCHAR(20),
            disbursement_date DATE,
            property_id VARCHAR(36) REFERENCES properties(property_id),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS transactions (
            incremental_id SERIAL,
            transaction_id VARCHAR(36) PRIMARY KEY,
            account_id VARCHAR(36) REFERENCES accounts(account_id),
            transaction_type VARCHAR(20),
            amount DECIMAL(15,2),
            direction VARCHAR(10),
            counterparty_key VARCHAR(255),
            counterparty_name VARCHAR(255),
            description TEXT,
            timestamp TIMESTAMP,
            status VARCHAR(20),
            pix_e2e_id VARCHAR(50),
            pix_key_type VARCHAR(20),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS card_transactions (
            incremental_id SERIAL,
            transaction_id VARCHAR(36) PRIMARY KEY,
            card_id VARCHAR(36) REFERENCES credit_cards(card_id),
            merchant_name VARCHAR(255),
            merchant_category VARCHAR(100),
            mcc_code VARCHAR(10),
            amount DECIMAL(15,2),
            installments INTEGER,
            timestamp TIMESTAMP,
            status VARCHAR(20),
            location_city VARCHAR(100),
            location_country VARCHAR(10),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );

        CREATE TABLE IF NOT EXISTS installments (
            incremental_id SERIAL,
            installment_id VARCHAR(36) PRIMARY KEY,
            loan_id VARCHAR(36) REFERENCES loans(loan_id),
            installment_number INTEGER,
            due_date DATE,
            principal_amount DECIMAL(15,2),
            interest_amount DECIMAL(15,2),
            total_amount DECIMAL(15,2),
            paid_date DATE,
            paid_amount DECIMAL(15,2),
            status VARCHAR(20),
            created_at TIMESTAMP,
            updated_at TIMESTAMP
        );
        """
        with self.conn.cursor() as cur:
            cur.execute(ddl)
        self.conn.commit()
        logger.info("Database tables created")

    def disable_constraints(self) -> None:
        """Disable FK constraints for bulk loading.

        Uses session_replication_role = replica to skip FK checks and triggers.
        Call ``enable_constraints()`` after loading to re-enable.
        """
        with self.conn.cursor() as cur:
            cur.execute("SET session_replication_role = replica")
        self.conn.commit()
        logger.info("FK constraints disabled (session_replication_role = replica)")

    def enable_constraints(self) -> None:
        """Re-enable FK constraints after bulk loading."""
        with self.conn.cursor() as cur:
            cur.execute("SET session_replication_role = DEFAULT")
        self.conn.commit()
        logger.info("FK constraints re-enabled (session_replication_role = DEFAULT)")

    @staticmethod
    def _flatten_dataclass(record: Any) -> dict[str, Any]:
        """Convert a dataclass to a flat dict, inlining nested Address."""
        data: dict[str, Any] = {}
        for f in fields(record):
            val = getattr(record, f.name)
            if f.name == "address" and is_dataclass(val):
                for af in fields(val):
                    data[af.name] = getattr(val, af.name)
            else:
                data[f.name] = val
        return data

    @staticmethod
    def _flatten_dict(data: dict, entity_type: str) -> dict:
        """Flatten nested address in dict records for customers/properties."""
        if entity_type in ("customers", "properties") and "address" in data:
            address = data.pop("address", {})
            data.update(address)
        return data

    def _extract_row(self, record: Any, columns: list[str], entity_type: str) -> tuple:
        """Extract row values from a record.

        Uses direct attribute access instead of ``asdict()`` to avoid
        deep recursive copies (2-3x faster for dataclasses with nested objects).
        """
        if is_dataclass(record):
            data = self._flatten_dataclass(record)
        elif isinstance(record, dict):
            data = self._flatten_dict(record, entity_type)
        else:
            raise ValueError(f"Unsupported record type: {type(record)}")

        return tuple(
            v.value if isinstance(v, Enum) else v
            for col in columns
            for v in (data.get(col),)
        )
